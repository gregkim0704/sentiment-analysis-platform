#!/usr/bin/env python3
"""
í¬ë¡¤ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
import asyncio
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.crawlers.naver_crawler import NaverNewsCrawler
from app.crawlers.daum_crawler import DaumNewsCrawler
from app.crawlers.google_crawler import GoogleNewsCrawler
from app.crawlers.manager import get_crawling_manager
from app.core.database import get_db
from app.core.logging import setup_logging, logger
from database.models import Company


async def test_individual_crawlers():
    """ê°œë³„ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ê°œë³„ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ìš© íšŒì‚¬ ê°ì²´ ìƒì„±
    test_company = Company(
        id=1,
        name="ì‚¼ì„±ì „ì",
        stock_code="005930",
        industry="ì „ì/IT",
        is_active=True
    )
    
    crawlers = [
        ("ë„¤ì´ë²„ ë‰´ìŠ¤", NaverNewsCrawler()),
        ("ë‹¤ìŒ ë‰´ìŠ¤", DaumNewsCrawler()),
        ("êµ¬ê¸€ ë‰´ìŠ¤", GoogleNewsCrawler()),
    ]
    
    results = {}
    
    for name, crawler in crawlers:
        print(f"\nğŸ“‹ {name} í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        try:
            async with crawler:
                articles = await crawler.crawl_company_news(test_company, days_back=1)
                
                results[name] = {
                    "success": True,
                    "articles_count": len(articles),
                    "sample_titles": [article.title for article in articles[:3]]
                }
                
                print(f"âœ… {name} í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                
                # ìƒ˜í”Œ ê¸°ì‚¬ ì œëª© ì¶œë ¥
                if articles:
                    print("   ìƒ˜í”Œ ê¸°ì‚¬:")
                    for i, article in enumerate(articles[:3], 1):
                        print(f"   {i}. {article.title[:50]}...")
                
        except Exception as e:
            print(f"âŒ {name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results[name] = {
                "success": False,
                "error": str(e),
                "articles_count": 0
            }
    
    return results


async def test_crawling_manager():
    """í¬ë¡¤ë§ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” í¬ë¡¤ë§ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í…ŒìŠ¤íŠ¸ íšŒì‚¬ ì¡°íšŒ
        db = next(get_db())
        
        try:
            test_company = db.query(Company).filter(
                Company.name == "ì‚¼ì„±ì „ì",
                Company.is_active == True
            ).first()
            
            if not test_company:
                print("âŒ í…ŒìŠ¤íŠ¸ íšŒì‚¬ 'ì‚¼ì„±ì „ì'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                print("ğŸ’¡ ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”: python scripts/init_db.py")
                return {"success": False, "error": "í…ŒìŠ¤íŠ¸ íšŒì‚¬ ì—†ìŒ"}
            
            print(f"ğŸ“‹ íšŒì‚¬ ì •ë³´: {test_company.name} ({test_company.stock_code})")
            
            # í¬ë¡¤ë§ ë§¤ë‹ˆì €ë¡œ í…ŒìŠ¤íŠ¸
            manager = get_crawling_manager()
            result = await manager.crawl_company_news(test_company, days_back=1, db=db)
            
            if result.get("success"):
                print(f"âœ… í¬ë¡¤ë§ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                print(f"   - ìˆ˜ì§‘ëœ ê¸°ì‚¬: {result.get('articles_found', 0)}ê°œ")
                print(f"   - ì¤‘ë³µ ì œê±° í›„: {result.get('articles_unique', 0)}ê°œ")
                print(f"   - ì €ì¥ëœ ê¸°ì‚¬: {result.get('articles_saved', 0)}ê°œ")
                
                # ì†ŒìŠ¤ë³„ ê²°ê³¼
                source_results = result.get('source_results', {})
                for source, source_result in source_results.items():
                    status = "ì„±ê³µ" if source_result.get('success') else "ì‹¤íŒ¨"
                    articles = source_result.get('articles', 0)
                    print(f"   - {source}: {status} ({articles}ê°œ)")
                
                return result
            else:
                print(f"âŒ í¬ë¡¤ë§ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.get('error')}")
                return result
                
        finally:
            db.close()
            
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        return {"success": False, "error": str(e)}


async def test_crawling_status():
    """í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ í…ŒìŠ¤íŠ¸...")
    
    try:
        manager = get_crawling_manager()
        status = await manager.get_crawling_status()
        
        print("âœ… í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ ì„±ê³µ")
        print(f"   - ìµœê·¼ 24ì‹œê°„ ì‘ì—… ìˆ˜: {status.get('summary', {}).get('total_jobs', 0)}ê°œ")
        print(f"   - ì„±ê³µë¥ : {status.get('summary', {}).get('success_rate', 0):.1f}%")
        print(f"   - ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {status.get('summary', {}).get('total_articles', 0)}ê°œ")
        
        return status
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}


def test_database_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸...")
    
    try:
        db = next(get_db())
        
        try:
            # íšŒì‚¬ ìˆ˜ ì¡°íšŒ
            company_count = db.query(Company).count()
            active_companies = db.query(Company).filter(Company.is_active == True).count()
            
            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            print(f"   - ì „ì²´ íšŒì‚¬ ìˆ˜: {company_count}ê°œ")
            print(f"   - í™œì„± íšŒì‚¬ ìˆ˜: {active_companies}ê°œ")
            
            if active_companies == 0:
                print("âš ï¸  í™œì„± íšŒì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
                return False
            
            return True
            
        finally:
            db.close()
            
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ê³  ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”:")
        print("   docker-compose up -d db")
        print("   python scripts/init_db.py")
        return False


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ë©€í‹° ìŠ¤í…Œì´í¬í™€ë” ì„¼í‹°ë©˜íŠ¸ ë¶„ì„ í”Œë«í¼ - í¬ë¡¤ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ë¡œê¹… ì„¤ì •
    setup_logging()
    
    # 1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
    if not test_database_connection():
        print("\nâŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return 1
    
    # 2. ê°œë³„ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
    crawler_results = await test_individual_crawlers()
    
    # 3. í¬ë¡¤ë§ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸
    manager_result = await test_crawling_manager()
    
    # 4. í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ í…ŒìŠ¤íŠ¸
    status_result = await test_crawling_status()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    # í¬ë¡¤ëŸ¬ë³„ ê²°ê³¼
    print("ğŸ“Š í¬ë¡¤ëŸ¬ë³„ ê²°ê³¼:")
    total_articles = 0
    successful_crawlers = 0
    
    for name, result in crawler_results.items():
        if result["success"]:
            successful_crawlers += 1
            articles = result["articles_count"]
            total_articles += articles
            print(f"   âœ… {name}: {articles}ê°œ ê¸°ì‚¬")
        else:
            print(f"   âŒ {name}: {result['error']}")
    
    print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
    print(f"   - ì„±ê³µí•œ í¬ë¡¤ëŸ¬: {successful_crawlers}/{len(crawler_results)}ê°œ")
    print(f"   - ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {total_articles}ê°œ")
    
    # ë§¤ë‹ˆì € ê²°ê³¼
    if manager_result.get("success"):
        print(f"   - ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸: âœ… ì„±ê³µ ({manager_result.get('articles_saved', 0)}ê°œ ì €ì¥)")
    else:
        print(f"   - ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸: âŒ ì‹¤íŒ¨")
    
    # ì „ì²´ ì„±ê³µ ì—¬ë¶€
    all_success = (
        successful_crawlers > 0 and 
        manager_result.get("success", False)
    )
    
    if all_success:
        print("\nğŸ‰ í¬ë¡¤ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. Celery ì›Œì»¤ ì‹œì‘: celery -A app.tasks.celery_app worker --loglevel=info")
        print("   2. í¬ë¡¤ë§ API í…ŒìŠ¤íŠ¸: python scripts/test_api.py")
        print("   3. ì›¹ ì¸í„°í˜ì´ìŠ¤ì—ì„œ í¬ë¡¤ë§ ì‹œì‘")
        return 0
    else:
        print("\nâš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("   - ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”")
        print("   - ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì ‘ê·¼ì´ ì°¨ë‹¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("   - VPN ì‚¬ìš©ì„ ê³ ë ¤í•´ë³´ì„¸ìš”")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)